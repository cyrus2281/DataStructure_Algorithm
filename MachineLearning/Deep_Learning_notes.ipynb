{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNaTdSq/NDv2y8DfWZkv0Hs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/DataStructure_Algorithm/blob/main/MachineLearning/Deep_Learning_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compiling a Model"
      ],
      "metadata": {
        "id": "EhZfLrKulBpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "\n",
        "resources:\n",
        "* https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/"
      ],
      "metadata": {
        "id": "FbgEgkqUfSsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Loss Functions"
      ],
      "metadata": {
        "id": "iOYd6N6EfWq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Square Error Loss (MSE)\n",
        "\n",
        "The Mean Squared Error, or MSE, loss is the default loss to use for regression problems.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.\n",
        "\n",
        "keras: `mean_squared_error`"
      ],
      "metadata": {
        "id": "efhGe2hBfZeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  #### Mean Squared Logarithmic Error Loss\n",
        "\n",
        "  There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error.\n",
        "\n",
        "Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short.\n",
        "\n",
        "It has the effect of relaxing the punishing effect of large differences in large predicted values.\n",
        "\n",
        "As a loss measure, it may be more appropriate when the model is predicting unscaled quantities directly. Nevertheless, we can demonstrate this loss function using our simple regression problem.\n",
        "\n",
        "Keras: `mean_squared_logarithmic_error`"
      ],
      "metadata": {
        "id": "s0uhvs0dfy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Absolute Error Loss\n",
        "\n",
        "On some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value.\n",
        "\n",
        "The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is **more robust to outliers**. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "\n",
        "keras: `mean_absolute_error`"
      ],
      "metadata": {
        "id": "L9OFgTGggPew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Classification Loss Functions"
      ],
      "metadata": {
        "id": "_urvie6nhgbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary Cross-Entropy Loss\n",
        "\n",
        "Cross-entropy is the default loss function to use for binary classification problems.\n",
        "\n",
        "It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "keras: `binary_crossentropy`\n",
        "\n",
        "Requires an output activation layer of `sigmoid`"
      ],
      "metadata": {
        "id": "KJcaCabCkRz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hinge Loss\n",
        "An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models.\n",
        "\n",
        "It is intended for use with binary classification where the target values are in the set **{-1, 1}**.\n",
        "\n",
        "The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.\n",
        "\n",
        "Reports of performance with the hinge loss are mixed, sometimes resulting in better performance than cross-entropy on binary classification problems.\n",
        "\n",
        "keras: `hinge`\n",
        "\n",
        "The output layer of the network must be configured to have a single node with a hyperbolic tangent activation function (keras: `tanh`) capable of outputting a single value in the range [-1, 1]."
      ],
      "metadata": {
        "id": "n7sxDb-dmA4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Squared Hinge Loss\n",
        "The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "A popular extension is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of **smoothing the surface** of the error function and making it numerically easier to work with.\n",
        "\n",
        "If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate.\n",
        "\n",
        "As with using the hinge loss function, the target variable must be modified to have values in the set {-1, 1}.\n",
        "\n",
        "keras: `squared_hinge`\n",
        "\n",
        "The output layer must use a single node with a hyperbolic tangent activation function (keras: `tanh`) capable of outputting continuous values in the range [-1, 1]."
      ],
      "metadata": {
        "id": "fu8QrIP4rQO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Class Classification Loss Functions"
      ],
      "metadata": {
        "id": "0A9XLcElsZUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-Class Cross-Entropy Loss\n",
        "Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, â€¦, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "keras: `categorical_crossentropy`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a `softmax` activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "X61eXdGDx2fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sparse Multiclass Cross-Entropy Loss\n",
        "A possible cause of frustration when using cross-entropy with classification problems with a **large number of labels** is the one hot encoding process.\n",
        "\n",
        "For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "keras: `sparse_categorical_crossentropy`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a `softmax` activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "bXr8oJ9x0C_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kullback Leibler Divergence Loss\n",
        "Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "As such, the KL divergence loss function is more commonly used **when using models that learn to approximate a more complex function** than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "Keras: `kullback_leibler_divergence`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a softmax activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "GNMDVkZQ0sZz"
      }
    }
  ]
}