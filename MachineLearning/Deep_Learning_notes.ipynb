{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqrf2ZEsGtnqF9s28f9qS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/DataStructure_Algorithm/blob/main/MachineLearning/Deep_Learning_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compiling a Model"
      ],
      "metadata": {
        "id": "EhZfLrKulBpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "FbgEgkqUfSsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Loss Functions"
      ],
      "metadata": {
        "id": "iOYd6N6EfWq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Square Error Loss (MSE)\n",
        "\n",
        "The Mean Squared Error, or MSE, loss is the default loss to use for regression problems.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.\n",
        "\n",
        "keras: `mean_squared_error`"
      ],
      "metadata": {
        "id": "efhGe2hBfZeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  #### Mean Squared Logarithmic Error Loss\n",
        "\n",
        "  There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error.\n",
        "\n",
        "Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short.\n",
        "\n",
        "It has the effect of relaxing the punishing effect of large differences in large predicted values.\n",
        "\n",
        "As a loss measure, it may be more appropriate when the model is predicting unscaled quantities directly. Nevertheless, we can demonstrate this loss function using our simple regression problem.\n",
        "\n",
        "Keras: `mean_squared_logarithmic_error`"
      ],
      "metadata": {
        "id": "s0uhvs0dfy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Absolute Error Loss\n",
        "\n",
        "On some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value.\n",
        "\n",
        "The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is **more robust to outliers**. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "\n",
        "keras: `mean_absolute_error`"
      ],
      "metadata": {
        "id": "L9OFgTGggPew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Classification Loss Functions"
      ],
      "metadata": {
        "id": "_urvie6nhgbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary Cross-Entropy Loss\n",
        "\n",
        "Cross-entropy is the default loss function to use for binary classification problems.\n",
        "\n",
        "It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "keras: `binary_crossentropy`\n",
        "\n",
        "Requires an output activation layer of `sigmoid`"
      ],
      "metadata": {
        "id": "KJcaCabCkRz8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-to_dzQkkcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}