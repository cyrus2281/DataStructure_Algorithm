{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMghmzTFDh27vl/Rb8We06B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/DataStructure_Algorithm/blob/main/MachineLearning/Deep_Learning_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table of Contents"
      ],
      "metadata": {
        "id": "j5-omnpm4t_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Preprocessing](#scrollTo=xsNekpZKBOEc)\n",
        "\n",
        ">>[Normalization](#scrollTo=7p33kYVQBRGQ)\n",
        "\n",
        ">>>[Scale](#scrollTo=E9zLAgPcCDSo)\n",
        "\n",
        ">>>>[MinMaxScaler](#scrollTo=Ok0T-s4sEQd1)\n",
        "\n",
        ">>>[Standardize](#scrollTo=PrKtPYkGCDMb)\n",
        "\n",
        ">>>>[RobustScaler](#scrollTo=4wuCZztWEjZl)\n",
        "\n",
        ">>>>[StandardScaler](#scrollTo=wgcHnXy9E5Y6)\n",
        "\n",
        ">>>>[Normalizer](#scrollTo=jcWMNwX9FurO)\n",
        "\n",
        ">[Neural Network](#scrollTo=ELNsz_33314I)\n",
        "\n",
        ">>[Activation Functions](#scrollTo=_s8F6y7T46-Q)\n",
        "\n",
        ">>>[Softmax Function](#scrollTo=jWjrCgx84_mH)\n",
        "\n",
        ">[Compiling a Model](#scrollTo=EhZfLrKulBpn)\n",
        "\n",
        ">>[Loss Function](#scrollTo=FbgEgkqUfSsW)\n",
        "\n",
        ">>>[Regression Loss Functions](#scrollTo=iOYd6N6EfWq_)\n",
        "\n",
        ">>>>[Mean Square Error Loss (MSE)](#scrollTo=efhGe2hBfZeV)\n",
        "\n",
        ">>>>[Mean Squared Logarithmic Error Loss](#scrollTo=s0uhvs0dfy00)\n",
        "\n",
        ">>>>[Mean Absolute Error Loss](#scrollTo=L9OFgTGggPew)\n",
        "\n",
        ">>>[Binary Classification Loss Functions](#scrollTo=_urvie6nhgbR)\n",
        "\n",
        ">>>>[Binary Cross-Entropy Loss](#scrollTo=KJcaCabCkRz8)\n",
        "\n",
        ">>>>[Hinge Loss](#scrollTo=n7sxDb-dmA4v)\n",
        "\n",
        ">>>>[Squared Hinge Loss](#scrollTo=fu8QrIP4rQO8)\n",
        "\n",
        ">>>[Multi-Class Classification Loss Functions](#scrollTo=0A9XLcElsZUr)\n",
        "\n",
        ">>>>[Multi-Class Cross-Entropy Loss](#scrollTo=X61eXdGDx2fP)\n",
        "\n",
        ">>>>[Sparse Multiclass Cross-Entropy Loss](#scrollTo=bXr8oJ9x0C_Z)\n",
        "\n",
        ">>>>[Kullback Leibler Divergence Loss](#scrollTo=GNMDVkZQ0sZz)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "mnQ2U85-3xLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "xsNekpZKBOEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "\n",
        "resources:\n",
        "* https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02"
      ],
      "metadata": {
        "id": "7p33kYVQBRGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize can be used to mean either `scale` or `standardize` (or even more!). Avoid the term normalize, because it has many definitions and is prone to creating confusion.\n",
        "Many machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed.\n",
        "\n",
        "Examples of such algorithm families include:\n",
        "* linear and logistic regression\n",
        "* nearest neighbors\n",
        "* neural networks\n",
        "* support vector machines with radial bias kernel functions\n",
        "* principal components analysis\n",
        "* linear discriminant analysis\n",
        "\n",
        "MinMaxScaler, RobustScaler, StandardScaler, and Normalizer are scikit-learn methods to preprocess data for machine learning.\n"
      ],
      "metadata": {
        "id": "khYRxiuGBldY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale\n",
        "Scale generally means to change the range of the values. The shape of the distribution doesn’t change. Think about how a scale model of a building has the same proportions as the original, just smaller. That’s why we say it is drawn to scale. The range is often set at 0 to 1."
      ],
      "metadata": {
        "id": "E9zLAgPcCDSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MinMaxScaler\n",
        "For each value in a feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. The range is the difference between the original maximum and original minimum.\n",
        "\n",
        "MinMaxScaler preserves the shape of the original distribution. It doesn’t meaningfully change the information embedded in the original data.\n",
        "\n",
        "Note that MinMaxScaler **doesn’t reduce the importance of outliers**. It’s non-distorting.\n",
        "\n",
        "The default range for the feature returned by MinMaxScaler is 0 to 1."
      ],
      "metadata": {
        "id": "Ok0T-s4sEQd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize\n",
        "Standardize generally means changing the values so that the distribution’s standard deviation equals one. Scaling is often implied."
      ],
      "metadata": {
        "id": "PrKtPYkGCDMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RobustScaler\n",
        "RobustScaler transforms the feature vector by subtracting the median and then dividing by the interquartile range (75% value — 25% value).\n",
        "\n",
        "Note that RobustScaler does not scale the data into a predetermined interval like MinMaxScaler. It does not meet the strict definition of *scale*.\n",
        "\n",
        "Use RobustScaler if you want to **reduce the effects of outliers**, relative to MinMaxScaler."
      ],
      "metadata": {
        "id": "4wuCZztWEjZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### StandardScaler\n",
        "StandardScaler is the industry’s go-to algorithm.\n",
        "\n",
        "StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler does not meet the strict definition of *scale*.\n",
        "\n",
        "StandardScaler results in a distribution with a standard deviation equal to 1. The variance is equal to 1 also, because variance = standard deviation squared. And 1 squared = 1.\n",
        "\n",
        "StandardScaler makes the mean of the distribution approximately 0.\n",
        "\n",
        "Deep learning algorithms often call for zero mean and unit variance. Regression-type algorithms also benefit from normally distributed data with small sample sizes.\n",
        "\n",
        "Use StandardScaler if you want each feature to have zero-mean, unit standard-deviation. If you want more normally distributed data, and are okay with transforming your data"
      ],
      "metadata": {
        "id": "wgcHnXy9E5Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalizer\n",
        "Normalizer works on the rows, not the columns! I find that very unintuitive. It’s easy to miss this information in the docs.\n",
        "\n",
        "By default, L2 normalization is applied to each observation so the that the values in a row have a unit norm. Unit norm with L2 means that if each element were squared and summed, the total would equal 1. Alternatively, L1 (aka taxicab or Manhattan) normalization can be applied instead of L2 normalization.\n",
        "\n",
        "Normalizer does transform all the features to values between -1 and 1"
      ],
      "metadata": {
        "id": "jcWMNwX9FurO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "ELNsz_33314I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions"
      ],
      "metadata": {
        "id": "_s8F6y7T46-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Function\n",
        "\n",
        "The softmax function, also known as softargmax or normalized exponential function, converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n",
        "\n",
        "The softmax function takes as input a vector z of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, `each component will be in the interval (0,1), and the components will add up to 1`, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "jWjrCgx84_mH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard (unit) softmax function $ \\sigma: \\mathbb{R}^K \\rightarrow (0,1)^K $ is defined when $K \\geq 1$ by the formula:\n",
        "\n",
        "\n",
        "$$\n",
        "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}} \\text{ for } i = 1,...,K \\text{ and } z= (z_1,...,z_K) \\in \\mathbb{R}^K\n",
        "$$"
      ],
      "metadata": {
        "id": "kUOjGwUq6wpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple words, it applies the standard exponential function to each element $z_{i}$ of the input vector $z$ and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector $\\sigma (\\mathbf {z} )$ is 1. The term \"softmax\" derives from the amplifying effects of the exponential on any maxima in the input vector.\n",
        "\n",
        "For example, the standard softmax of (1,2,8) is approximately (0.001,0.002,0.997), which amounts to assigning almost all of the total unit weight in the result to the position of the vector's maximal element (of 8).\n",
        "\n",
        "\\\\\n",
        "\n",
        "The softmax function is used in various `multiclass classification methods`"
      ],
      "metadata": {
        "id": "L6adeqO8-tR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compiling a Model"
      ],
      "metadata": {
        "id": "EhZfLrKulBpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "\n",
        "resources:\n",
        "* https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/"
      ],
      "metadata": {
        "id": "FbgEgkqUfSsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Loss Functions"
      ],
      "metadata": {
        "id": "iOYd6N6EfWq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Square Error Loss (MSE)\n",
        "\n",
        "The Mean Squared Error, or MSE, loss is the default loss to use for regression problems.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.\n",
        "\n",
        "keras: `mean_squared_error`"
      ],
      "metadata": {
        "id": "efhGe2hBfZeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  #### Mean Squared Logarithmic Error Loss\n",
        "\n",
        "  There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error.\n",
        "\n",
        "Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short.\n",
        "\n",
        "It has the effect of relaxing the punishing effect of large differences in large predicted values.\n",
        "\n",
        "As a loss measure, it may be more appropriate when the model is predicting unscaled quantities directly. Nevertheless, we can demonstrate this loss function using our simple regression problem.\n",
        "\n",
        "Keras: `mean_squared_logarithmic_error`"
      ],
      "metadata": {
        "id": "s0uhvs0dfy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean Absolute Error Loss\n",
        "\n",
        "On some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value.\n",
        "\n",
        "The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is **more robust to outliers**. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "\n",
        "keras: `mean_absolute_error`"
      ],
      "metadata": {
        "id": "L9OFgTGggPew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Classification Loss Functions"
      ],
      "metadata": {
        "id": "_urvie6nhgbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary Cross-Entropy Loss\n",
        "\n",
        "Cross-entropy is the default loss function to use for binary classification problems.\n",
        "\n",
        "It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "keras: `binary_crossentropy`\n",
        "\n",
        "Requires an output activation layer of `sigmoid`"
      ],
      "metadata": {
        "id": "KJcaCabCkRz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hinge Loss\n",
        "An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models.\n",
        "\n",
        "It is intended for use with binary classification where the target values are in the set **{-1, 1}**.\n",
        "\n",
        "The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.\n",
        "\n",
        "Reports of performance with the hinge loss are mixed, sometimes resulting in better performance than cross-entropy on binary classification problems.\n",
        "\n",
        "keras: `hinge`\n",
        "\n",
        "The output layer of the network must be configured to have a single node with a hyperbolic tangent activation function (keras: `tanh`) capable of outputting a single value in the range [-1, 1]."
      ],
      "metadata": {
        "id": "n7sxDb-dmA4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Squared Hinge Loss\n",
        "The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "A popular extension is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of **smoothing the surface** of the error function and making it numerically easier to work with.\n",
        "\n",
        "If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate.\n",
        "\n",
        "As with using the hinge loss function, the target variable must be modified to have values in the set {-1, 1}.\n",
        "\n",
        "keras: `squared_hinge`\n",
        "\n",
        "The output layer must use a single node with a hyperbolic tangent activation function (keras: `tanh`) capable of outputting continuous values in the range [-1, 1]."
      ],
      "metadata": {
        "id": "fu8QrIP4rQO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Class Classification Loss Functions"
      ],
      "metadata": {
        "id": "0A9XLcElsZUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-Class Cross-Entropy Loss\n",
        "Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "keras: `categorical_crossentropy`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a `softmax` activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "X61eXdGDx2fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sparse Multiclass Cross-Entropy Loss\n",
        "A possible cause of frustration when using cross-entropy with classification problems with a **large number of labels** is the one hot encoding process.\n",
        "\n",
        "For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "keras: `sparse_categorical_crossentropy`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a `softmax` activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "bXr8oJ9x0C_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kullback Leibler Divergence Loss\n",
        "Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "As such, the KL divergence loss function is more commonly used **when using models that learn to approximate a more complex function** than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "Keras: `kullback_leibler_divergence`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a softmax activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "GNMDVkZQ0sZz"
      }
    }
  ]
}